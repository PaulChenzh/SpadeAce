1. 什么是Tick Data
Tick Data本身并不神秘，就是交易所把每只股票（亦或是futures options）的active order book(就是你的委托还存在在交易所里面，但并且没有被撮合成交)里面的买、卖的单的情况发给你。
Tickey

2. HDF(Hierarchical Data Format),可以存储不同类型的图像和数码数据的文件格式，并且可以在不同类型的机器上传输，同时还有统一处理这种文件格式的函数库。
HDF5

3.kdb 是专门用于执行系统转储映像分析的实用工具
多用于Quant
https://www.ibm.com/developerworks/cn/aix/library/au-kdbsteps/

4.深度学习
https://zhuanlan.zhihu.com/p/24768878?refer=zhangbingyang

5.AI情况调研
http://www.sohu.com/a/138305708_465975

20170706
1.PCA 主成分分析
https://my.oschina.net/gujianhan/blog/225241

2.Deep Learning
http://deeplearning.stanford.edu/wiki/index.php/UFLDL%E6%95%99%E7%A8%8B
https://zhuanlan.zhihu.com/p/24768878?refer=zhangbingyang
http://www.cnblogs.com/tornadomeet/tag/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/default.html?page=1

20170707
1.二元线性回归
http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=DeepLearning&doc=exercises/ex2/ex2.html

2.卷积神经网络（CNN，Convolutional Neural Network）
http://www.cnblogs.com/GarfieldEr007/p/5342906.html

3.深度学习导论
这里，先推荐一篇非常不错的文章： 《1天搞懂深度学习》，300多页的ppt，台湾李宏毅教授写的，非常棒。 不夸张地说，是我看过最系统，也最通俗易懂的，关于深度学习的文章。

这是slideshare的链接： http://www.slideshare.net/tw_dsconf/ss-62245351?qid=108adce3-2c3d-4758-a830-95d0a57e46bc&amp;v=&amp;b=&amp;from_search=3

作者：jacky yang
链接：https://www.zhihu.com/question/26006703/answer/129209540
来源：知乎
著作权归作者所有，转载请联系作者获得授权。


4.
这位爱好者把151只精灵宝可梦都做成了《炉石传说》卡牌
http://link.zhihu.com/?target=https%3A//imgur.com/a/hbNxg

5.十个值得一试的开源深度学习框架
http://www.oschina.net/news/68074/ten-worth-a-try-open-deep-learning-framework

20170710
1. TensorFlow - Github
https://github.com/tensorflow/tensorflow
2. TensorFlow - 中文网站
http://www.tensorfly.cn/
3.负载均衡器
Nginx - 软件
F5 - 硬件
http://www.cnblogs.com/brookzhang/p/6600861.html
4. 优雅降级(Graceful degradation)是指电脑，机器，电子系统或者是网络在本身大部分已经毁坏或无效的情况下还能保持有限的功能这种能力。优雅降级的目的是阻止灾难性的失败。理想情况下，有优雅降级特征的系统即使多个组件同时失效也不会引起停机。在优雅降级中，操作的效率和速度随着失效部件的增加逐渐下降。
5.大型网站架构系列
http://www.cnblogs.com/itfly8/p/5155983.html
https://www.zhihu.com/topic/19570823/top-answers
6.常用消息队列
一般商用的容器，比如WebLogic，JBoss，都支持JMS标准，开发上很方便。但免费的比如Tomcat，Jetty等则需要使用第三方的消息中间件。本部分内容介绍常用的消息中间件（Active MQ,Rabbit MQ，Zero MQ,Kafka）以及他们的特点。
7.Spring2.0的特性是什么？
8.告诉你Hadoop是什么
http://os.51cto.com/art/201207/346023.htm
9.Hadoop开篇必读
http://blog.fens.me/hadoop-family-roadmap/
10.DataCastle
秋小白
2 人赞同了该回答
去做DataCastle上面的竞赛题吧~
一般都是公司在实际运作中遇到的问题，然后放到这个竞赛平台上来让专业的人解决
在做竞赛中才能知道哪些需要更多的补充
11.开源机器学习框架
http://www.dataguru.cn/thread-601710-1-1.html
12.CTR（Click-Through-Rate）即点击通过率，是互联网广告常用的术语，指网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的实际点击次数（严格的来说，可以是到达目标页面的数量）除以广告的展现量（Show content）。
13.机器学习面试的那些事儿
https://zhuanlan.zhihu.com/p/22387312
14.机器学习面试
https://www.zhihu.com/question/23259302
15.从0开始TensorFlow
https://www.leiphone.com/news/201606/ORlQ7uK3TIW8xVGF.html

20170711
1.round-robin 轮询调度
2.显式分布式架构
3.kafka
http://blog.jobbole.com/75328/
https://zhuanlan.zhihu.com/p/25212966
http://www.cnblogs.com/likehua/p/3999538.html
Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消费。
-- what is Kafka
Kafka是可靠的分布式日志存储服务。
用简单的话来说，你可以把Kafka当作可顺序写入的一大卷磁带， 可以随时倒带，快进到某个时间点重放。
作者：傅理
链接：https://www.zhihu.com/question/22480085/answer/135092014
来源：知乎
著作权归作者所有，转载请联系作者获得授权。

Kafka的特征如下：

高写入速度：Kafka能以超过1Gbps NIC的速度写这盘磁带（实际可以到SATA 3速度，参考Benchmarking Apache Kafka: 2 Million Writes Per Second (On Three Cheap Machines)
)，充分利用了磁盘的物理特性，即，随机写入慢（磁头冲停），顺序写入快（磁头悬浮）。

高可靠性： 通过zookeeper做分布式一致性，同步到任意多块磁盘上，故障自动切换选主，自愈。

高容量：通过横向扩展，LinkedIn每日通过Kafka存储的新增数据高达175TB，8000亿条消息，可无限扩容，类似把两条磁带粘到一起。

传统业务数据库的根本缺陷在于：
1. 太慢，读写太昂贵，无法避免的随机寻址。（磁盘最快5ms寻址，固态又太昂贵。）
2. 根本无法适应持续产生的数据流，越用越慢。（索引效率问题）
3. 无法水平scale。（多半是读写分离，一主多备。另: NewSQL通过一致性算法，有多主。）

针对这些问题，Kafka提出了一种方法: 

“log-centric approach（以日志为中心的方法）。”

将传统数据库分为两个独立的系统，即日志系统和索引系统。

“持久化和索引分开，日志尽可能快的落地，索引按照自己的速度追赶。”

在数据可靠性在得到Kafka这种快速的，类似磁带顺序记录方式保障的大前提下。数据的呈现，使用方式变得非常灵活，可以根据需要将数据流同时送入搜索系统，RDBMS系统，数据仓库系统， 图数据库系统，日志分析等这些各种不同的数据库系统。 这些不同的系统只不过是一种对Kafka磁带数据的一种诠释，一个侧面，一个索引，一个快照。数据丢了，没关系，重放一遍磁带即可，更多的时候，对这些各式数据库系统的维护只是需要定期做一个快照，并拷贝到一个安全的对象存储(如S3) 而已。 

一句话：“日志都是相同的日志，索引各有各的不同。”

以上故事说明了Kafka主要用途是数据集成，或者说是流数据集成，以Pub/Sub形式的消息总线形式提供。但是，Kafka不仅仅是一套传统的消息总线，本质上Kafka是分布式的流数据平台，因为以下特性而著名：

提供Pub/Sub方式的海量消息处理。
以高容错的方式存储海量数据流。
保证数据流的顺序。
4.Hadoop/Spark
Hadoop -> HDFS分布式数据存储 + MapReduce
	Hadoop解决了大数据（大到一台计算机无法进行存储，一台计算机无法在要求的时间内进行处理）的可靠存储和处理。
		HDFS，在由普通PC组成的集群上提供高可靠的文件存储，通过将块保存多个副本的办法解决服务器或硬盘坏掉的问题。
		MapReduce，通过简单的Mapper和Reducer的抽象提供一个编程模型，
		Apache Pig解决了MapRedude存在的大量手写代码，语义隐藏，提供操作种类少的问题，类似的项目还有Cascading,JAQL等。
Spark-大数据处理引擎，主要特点是提供了一个集群的分布式内存抽象。

Spark没有文件管理系统，所以，它必须和其他的分布式文件系统进行集成才能运作。
但Spark数据处理速度秒杀MapReduce。Spark的批处理速度比MapReduce快近10倍，内存中的数据分析速度则快近100倍。
-> Spark处理+Hadoop的HDFS分布式数据存储
what is MapReduce?
我们要数图书馆中的所有书。你数1号书架，我数2号书架。这就是“Map”。我们人越多，数书就更快。
现在我们到一起，把所有人的统计数加在一起。这就是“Reduce”。

深入理解Hadoop集群
http://os.51cto.com/art/201211/364374.htm

5.Paul got promoted to be an assistant manager in 2017. He proved himself for the qualification with good work performance. Job well done and looking forward to seeing him more success in 2017. 

20170712
1.如何快速成为数据分析师
https://www.zhihu.com/question/29265587

20170717
1. Spark实战
http://www.cnblogs.com/shishanyuan/p/4699644.html
	1.1 一个三个节点的集群
	a.操作系统：CentOS6.5 64bit
	b.Linux传输工具:SSH Secure File Transfer
	c.Linux命令行执行工具：SSH Secure Shell, SecureCRT
	2.配置运行环境
	a. 更新OpenSSL #yum update openssl
	b. 修改SSH配置文件
	c. 增加 hadoop组和用户
	d. JDK1.7 http://www.oracle.com/technetwork/java/javase/downloads/jdk7-downloads-1880260.html
	打开界面之后，先选中 Accept License Agreement ，然后下载 jdk-7u55-linux-x64.tar.gz，如下图所示：
	e. Scala安装包 Scala2.10.4安装包下载链接为：http://www.scala-lang.org/download/2.10.4.html
	3.配置集群环境
	4....
2. 编译Hadoop
	1.环境配置
		1.maven http://mirror.bit.edu.cn/apache/maven/maven-3/
		2.SVN #yum install svn
		3.autoconf automake libtool cmake #yum install autoconf automake libtool cmake
		4.ncurses-devel #yum install ncurses-devel
		5.openssl-devel #yum install openssl-devel
		6.gcc* #yum install gcc*
		7.protobuf https://code.google.com/p/protobuf/downloads/list
	2.编译Hadoop
		1.Hadoop源码 Release2.2.0 $svn checkout http://svn.apache.org/repos/asf/hadoop/common/tags/release-2.2.0
		2.编译Hadoop源码 $mvn package -Pdist,native -DskipTests –Dtar
		3.验证编译是否成功
	3.安装Hadoop
		1.在主节点上压缩 
		$cd /home/hadoop/upload/
		$tar -xzf hadoop-2.2.0.tar.gz
		2. 把hadoop-2.2.0目录移到/app/hadoop目录下
2. Storm实时分布式系统

20170724
1. 解决mac问题

20170725
1. 阿里面经
http://www.jianshu.com/p/5681a1f0aad6
2.JAVA转大数据
http://www.cnblogs.com/jasonzeng888/p/6231675.html
3.想要自己尝试写一个大数据的项目
-个人的一点想法
<1>先在win,ubuntu和macOC上面，分别把hadoop搭建起来 - ing
<2>再写一个爬虫，抓一些数据，用来分析
<3>再进一步做一些自己的分析...
<4>...
1)可以尝试自己写一些项目，收集一些数据然后自己分析，之前有用Python爬取学校贴吧用户名和性别，分析男女比例，所以还是推荐数据分析的话学Python吧，最后安利最近的一个超级无敌简单的小项目TXES邮件提醒系统 http://tomxin.cnWeb端是JSP+HTML+Servlet开发的，超级简单的一个系统，整好可以了解整个数据的交互，当然还有安卓端和桌面端(Delphi开发)，后台是Python进行逻辑处理，SQL Server作为数据库。地址:http://tomxin.cn 网站里面有github地址
2)创建一个个人的博客
	个人博客搭建（wordpress+阿里云服务器:http://www.cnblogs.com/smyhvae/p/4965163.html
	服务器选择：http://www.chinaz.com/web/2011/0918/210568.shtml
		最好还是选linux+php
	服务器选择虚拟主机还是服务器：http://www.cnblogs.com/zysbk/p/3234547.html
	GoDaddy域名
	*1.找到蓝牙键盘的充电器
	*2.在iphone和mac上找一款能够对接的远程登录的软件,命令行控制
	*3.当个人站建好后，让其在公司也能访问
3)每做一件事情，都写一篇guide
4)在知乎上有自己的回答积累


作者：Tomxin7
链接：https://www.zhihu.com/question/21380122/answer/133937799
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

4.手机上载一个remote登陆的软件来玩
在电脑上安装一个对应的软件，试试看，可能不需要申请固定ip

在局域网中remove:http://jingyan.baidu.com/article/1876c852ad7db1890b1376d9.html
5.专有网络 (Virtual Private Cloud，简称VPC) ，能够帮助用户基于阿里云构建出一个隔离的网络环境。用户可以完全掌控自己的虚拟网络，包括选择自有 IP 地址范围、划分网段、配置路由表和网关等。此外用户可以通过专线/VPN等连接方式将VPC与传统数据中心组成一个按需定制的网络环境，实现应用的平滑迁移上云。
VPS可以将专有网络的私有IP地址范围分割成一个或多个虚拟交换机，根据需要将应用程序和其他服务部署在对应的虚拟交换机下。根据业务需求配置虚拟路由器的路由规则，管理专有网络流量的转发路径。VPS具有安全组功能，可以将专有网络中的产品实例划分成不同的安全域，并为每个安全域定义不同的访问控制规则。通过专有网络支持专线/VPN等多种连接方式，专有网络与物理网络或者不同专有网络之间可以实现连接，组成一个虚拟的混合网络。弹性公网IP（可以独立申请使用的公网IP地址）可以按需绑定到相同地域下专有网络类型的云产品实例上，绑定和解绑操作都即时生效。


20170726
1.在阿里云上配了一台1核2G的云服务器，9.5折约98每月
远程连接密码：456139


20170731
1. Linux命令大全
http://www.runoob.com/linux/linux-command-manual.html
2. Linux 技巧：让进程在后台可靠运行的几种方法
https://www.ibm.com/developerworks/cn/linux/l-cn-nohup/
--------------------------------------------------------------------------
1. Linux安装Hadoop
1)Hadoop2.6.5版本
http://apache.claz.org/hadoop/common/hadoop-2.6.5/hadoop-2.6.5-src.tar.gz 
2)通过wget下载hadoop
wget http://apache.claz.org/hadoop/common/hadoop-2.6.5/hadoop-2.6.5-src.tar.gz 
*/root目录就是~目录
3)解压缩hadoop
tar -zxcf /root/hadoop-2.6.5-src.tar.gz
2.安装JDK
1)JDK1.8
http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz
2)通过wget下载hadoop
wget http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.tar.gz
3)解压缩jdk
tar -zxvf jdk-8u144-linux-x64.tar.gz
	*Linux解压tar.gz文件时提示gzip：stdin：not错误
	gzip： stdin： not in gzip format
	tar： Child returned status 1
	tar： Error is not recoverable： exiting now
	第一个解决方案：http://alany.blog.51cto.com/6125308/1422299
	-->均无效
wget http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.rpm

----------插播--------------
重启服务器之后，执行：nginx -s reload
报如下错：
nginx: [error] open() "/run/nginx.pid" failed (2: No such file or directory)
	*linux操作系统重启后 解决nginx的pid消失问题:http://www.cnblogs.com/cyq632694540/p/6483535.html
找到nginx的启动程序/usr/sbin/nginx,nginx的配置文件/etc/nginx/nginx.conf,-c为以什么文件启动：
/usr/sbin/nginx -c /etc/nginx/nginx.conf


20170801
1. 继续安装Hadoop

2.启动部署
1）格式化nameNode
cd /app/hadoop/hadoop2.6.5/
./bin/hdfs namenode -format
2)启动HDFS
*HDFS的运行原理：http://www.cnblogs.com/laov/p/3434917.html
cd /app/hadoop/hadoop2.6.5/sbin
./start-dfs.sh
出现错误：Incorrect configuration: namenode address dfs.namenode.servicerpc-address or dfs.namenode.rpc-address is not configured.
3)检验HDFS是否启动
jps
30654 Jps
*Hadoop 新 MapReduce 框架 Yarn 详解：https://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop-yarn/
4）配置core-site.xml
*Hadoop core-site.xml 配置项列表：http://www.cnblogs.com/yinchengzhe/p/5000217.html
*《hadoop学习》关于hdfs中的core-site.xml,hdfs-site.xml,mapred-site.xml文件配置详解：http://www.cnblogs.com/forget-me-not/p/5710122.html




*教你如何做游戏：https://github.com/miloyip/game-programmer


20170802
1. yarn是什么
yarn-site.xml配置：http://www.cnblogs.com/captainlucky/p/4610295.html
2.yarn-env.sh配置：
yarn-env.sh在./etc/hadoop/目录下
vi yarn-env.sh
加入JAVA_HOME路径
export JAVA_HOME=/root/tools/jdk1.7.0_79
之后热加载该文件，使其生效：
source yarn-env.sh
3.董的博客：关注大数据处理http://dongxicheng.org/mapreduce-nextgen/hadoop-yarn-configurations-resourcemanager-nodemanager/
4.hadoop三个配置文件core-site.xml、hdfs-site.xml和mapred-site.xml
http://www.iyunv.com/thread-17698-1-1.html
5.core-site.xml配置
6.URI:统一资源标识符:https://www.zhihu.com/question/21950864
7.windows下安装hadoop：http://www.cnblogs.com/kinglau/archive/2013/08/20/3270160.html
8.写一篇Linux命令篇，一方面用于积累，一方面用于方便查询，一方面用于快速查询
9.写一篇CentOS搭建个人站点时遇到的问题
10.等CentOS搭建完Hadoop，写一篇有关公略+遇到的问题集合
11.将windows的hadoop build起来
12.将mac的hadoop build起来
